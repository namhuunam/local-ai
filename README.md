# C√ÄI ƒê·∫∂T OLLAMA TR√äN WINDOWS
1 V√†o https://ollama.ai/download/windows
2 Download file OllamaSetup.exe
3 Ch·∫°y installer v·ªõi quy·ªÅn Administrator
Ki·ªÉm tra GPU ƒë∆∞·ª£c detect:
```bash
ollama serve
```
X√≥a model c≈©:
```bash
ollama rm qwen:4b
```
Ki·ªÉm tra model n√†o ƒëang c√≥
```bash
ollama list
```
C√†i ƒë·∫∑t model:
```bash
ollama pull qwen2:7b-instruct
```
```bash
ollama pull mistral:7b-instruct
```
### 1 Download manual Windows:
1 V√†o https://github.com/cloudflare/cloudflared/releases

2 Download cloudflared-windows-amd64.exe

3 ƒê·ªïi t√™n th√†nh cloudflared.exe

4 Copy v√†o C:\Windows\System32\ (c·∫ßn quy·ªÅn Admin)

### 1.1 Download manual ubuntu:
```bash
wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
```
```bash
sudo dpkg -i cloudflared-linux-amd64.deb
```

### 2 T·ª´ Zero Trust Dashboard:

1 V√†o https://one.dash.cloudflare.com
2 Ch·ªçn Networks ‚Üí Tunnels

3 Ho·∫∑c Networks ‚Üí Cloudflare Tunnel

### 3 T·∫°o tunnel m·ªõi:

1 Click Create a tunnel

2 Ch·ªçn Cloudflared

3 ƒê·∫∑t t√™n tunnel: qwen-ai-tunnel

4 Click Save tunnel

### B∆Ø·ªöC 34: C√ÄI ƒê·∫∂T CONNECTOR

3.1 Copy install command:

Trong tunnel dashboard s·∫Ω c√≥ l·ªánh nh∆∞:

```bash
cloudflared.exe service install eyJhIjoiYWJjMTIzLi4uIn0=
```

3.2 Ch·∫°y l·ªánh v·ªõi quy·ªÅn Administrator:

```bash
cloudflared.exe service install eyJhIjoiYWJjMTIzLi4uIn0=
```

Ho·∫∑c n·∫øu ch∆∞a c√≥ cloudflared:

```bash
# Download v√† install
winget install --id Cloudflare.cloudflared

# R·ªìi ch·∫°y l·ªánh install
cloudflared.exe service install eyJhIjoiYWJjMTIzLi4uIn0=
```

### B∆Ø·ªöC 5: C·∫§U H√åNH PUBLIC HOSTNAME

4.1 Trong Tunnel Dashboard:

1 Ch·ªçn tab Public Hostnames

2 Click Add a public hostname

4.2 C·∫•u h√¨nh hostname:

 Subdomain: ai (ho·∫∑c qwen, ollama)
 
 Domain: codepion.xyz
 
 Path: ƒë·ªÉ tr·ªëng
 
 Service Type: HTTP
 
 URL: localhost:11434
 
K·∫øt qu·∫£: ai.codepion.xyz ‚Üí localhost:11434
HTTP Settings
HTTP Host Header:localhost
4.3 Click Save hostname

### B∆Ø·ªöC 6: TEST DOMAIN

6.1 ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y:

```bash
curl http://localhost:11434/api/tags
curl https://ai.codepion.xyz/api/tags
```

Auto Start Cloudflare Tunnel

```bash
cloudflared service install
```

```bash
sc config cloudflared start= auto
```

### L·ªánh ki·ªÉm tra ollama c√≥ s·ª≠ d·ª•ng GPU tr√™n WSL2 kh√¥ng
```bash
watch -n 0.5 nvidia-smi
```


### t·∫°o enpoint ri√™ng cho cpu v√† gpu
B∆∞·ªõc 1: T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c m·ªõi
```bash
# T·∫°o th∆∞ m·ª•c config ri√™ng cho m·ªói instance
mkdir -p /home/ubuntu/.ollama-cpu
mkdir -p /home/ubuntu/.ollama-gpu

# T·∫°o symlink ƒë·ªÉ share models (ti·∫øt ki·ªám dung l∆∞·ª£ng)
ln -s /home/ubuntu/.ollama/models /home/ubuntu/.ollama-cpu/models
ln -s /home/ubuntu/.ollama/models /home/ubuntu/.ollama-gpu/models

# Ki·ªÉm tra symlink ƒë√£ t·∫°o ƒë√∫ng ch∆∞a
ls -la /home/ubuntu/.ollama-cpu/
ls -la /home/ubuntu/.ollama-gpu/
```

```bash
nano ~/ollama-cpu.sh
```
```bash
#!/bin/bash
export CUDA_VISIBLE_DEVICES=""
export OLLAMA_HOST="127.0.0.1:11436"
export OLLAMA_MODELS="/home/ubuntu/.ollama-cpu"

echo "üñ•Ô∏è Starting Ollama CPU instance on port 11436..."
echo "Models path: $OLLAMA_MODELS"
echo "Shared models via symlink"
ollama serve &
wait
```

```bash
nano ~/ollama-gpu.sh
```
```bash
#!/bin/bash
export CUDA_VISIBLE_DEVICES="0"
export OLLAMA_HOST="127.0.0.1:11435"
export OLLAMA_MODELS="/home/ubuntu/.ollama-gpu"

echo "üöÄ Starting Ollama GPU instance on port 11435..."
echo "Models path: $OLLAMA_MODELS"
echo "Shared models via symlink"
ollama serve &
wait
```
```bash
# Pull cho CPU instance
OLLAMA_HOST="127.0.0.1:11436" ollama pull llama3:8b

# Pull cho GPU instance  
OLLAMA_HOST="127.0.0.1:11435" ollama pull qwen2:7b-instruct
```
B∆∞·ªõc 4: Kh·ªüi ƒë·ªông v√† test
```bash
./ollama-cpu.sh
```
```bash
./ollama-gpu.sh
```
B∆∞·ªõc 5: T·ª± Kh·ªüi ƒë·ªông
T·∫°o systemd service cho CPU
```bash
sudo nano /etc/systemd/system/ollama-cpu.service
```
```bash
[Unit]
Description=Ollama CPU Instance (llama3:8b)
After=network.target
Wants=network.target

[Service]
Type=simple
User=ubuntu
Group=ubuntu
WorkingDirectory=/home/ubuntu
ExecStart=/home/ubuntu/ollama-cpu.sh
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```
T·∫°o systemd service cho GPU
```bash
sudo nano /etc/systemd/system/ollama-gpu.service
```
```bash
[Unit]
Description=Ollama GPU Instance (qwen2:7b-instruct)
After=network.target ollama-cpu.service
Wants=network.target

[Service]
Type=simple
User=ubuntu
Group=ubuntu
WorkingDirectory=/home/ubuntu
ExecStart=/home/ubuntu/ollama-gpu.sh
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```
C·∫•p quy·ªÅn th·ª±c thi cho scripts
```bash
chmod +x ~/ollama-cpu.sh
chmod +x ~/ollama-gpu.sh
```
K√≠ch ho·∫°t services
```bash
# Reload systemd
sudo systemctl daemon-reload

# Enable auto-start khi boot
sudo systemctl enable ollama-cpu
sudo systemctl enable ollama-gpu

# Start services ngay
sudo systemctl start ollama-cpu
sudo systemctl start ollama-gpu

# Ki·ªÉm tra status
sudo systemctl status ollama-cpu
sudo systemctl status ollama-gpu
```
